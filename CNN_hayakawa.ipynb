{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "['title' 'title' 'ｏｖｅｒ\\u3000ｔｈｅ\\u3000ｅｄｇｅ | ジャンル,ミステリ | ハヤカワ・オンライン' ...,\n",
      " 'サルたちの狂宴 上 | ジャンル,伝記／評論 | ハヤカワ・オンライン' '甦る男 | 受賞作品 | ハヤカワ・オンライン'\n",
      " '人生は２０代で決まる | ジャンル,ノンフィクション | ハヤカワ・オンライン']\n",
      "None\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'a' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-6fc274e655e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vod.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'___META_MAXLEN___'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-6fc274e655e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_name\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0midx_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-6fc274e655e3>\u001b[0m in \u001b[0;36minit_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#行の最大文字数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#行数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'a' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    " \n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    " \n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import pickle\n",
    " \n",
    "# 適当なデータを用意してください\n",
    "df = pd.read_csv('data/book.csv', header=None)\n",
    "\n",
    "def only_str(word):\n",
    "    if type(word) is word:\n",
    "        return word\n",
    "    else:\n",
    "        pass\n",
    "#読み込んだcsvから長さ読み込んでもろもろの処理をするメソッドを書く\n",
    "def init_train():\n",
    "    print('Loading data')\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    voc = {}\n",
    "    maxlen = 0\n",
    "    maxwords = 0\n",
    "    buff = set()\n",
    "    TARGET_DIR = 'data/txt/*'\n",
    "    idx_name = {}\n",
    "    print(str(df[3].values))\n",
    "    for i,filename in enumerate(df[3].str.strip()):\n",
    "        idx_name[i] = filename\n",
    "    for line in df[2].str.strip():\n",
    "        print(only_str(line))\n",
    "        try:\n",
    "            a = list(only_str(line))\n",
    "        except:\n",
    "            pass\n",
    "    #行の最大文字数\n",
    "        maxlen = max(maxlen, len(a))\n",
    "        [buff.add(w) for w in a]\n",
    "    #行数\n",
    "    maxwords = len(buff)\n",
    "    #限界にMAX,それぞれの項目に数値を入れている\n",
    "    voc[maxwords] = '___MAX___'\n",
    "    voc['___META_MAXWORD___'] = maxwords\n",
    "    voc['___META_MAXLEN___'] = maxlen\n",
    "    print(\"maxwords %d\"%maxwords)\n",
    "    print(\"maxlen %d\"%maxlen)\n",
    "    print(\"idx name len %d\"%len(idx_name))\n",
    "    for i,filename in enumerate(df[3]):\n",
    "        for line in set(filter(lambda x:x!='',df[2])): \n",
    "            X = [maxwords]*maxlen\n",
    "            #余分な空白を消す\n",
    "            line = line.strip()\n",
    "            for idx, ch in enumerate(list(line)):\n",
    "            #辞書vocのch番目が殻だったらvocの長さを代入\n",
    "                if voc.get(ch) == None:\n",
    "                    voc[ch] = len(voc)\n",
    "                convert = voc[ch]\n",
    "                X[idx] = convert\n",
    "                #Xs,Ysそれぞれにvocの長さ,倍率を追加している\n",
    "        Xs.append(X)\n",
    "        y = [0.]*len(idx_name)\n",
    "        y[i] = 1.\n",
    "        Ys.append(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split( Xs, Ys, test_size=0.1, random_state=42)\n",
    "    print(voc)\n",
    "    open('vod.pkl', 'wb').write(pickle.dumps(voc))\n",
    "    open('idx_name.pkl', 'wb').write(pickle.dumps(idx_name))\n",
    "    sequence_length = maxlen\n",
    "    vocabulary_size = maxwords\n",
    "    embedding_dim   = 256*1\n",
    "    filter_sizes    = [3,4,5,1,2]\n",
    "    num_filters     = 512*1\n",
    "    drop            = 0.5\n",
    "    \n",
    "    nb_epoch   = 10\n",
    "    batch_size = 30\n",
    "    return sequence_length, embedding_dim, filter_sizes, vocabulary_size, num_filters, drop, idx_name, \\\n",
    "    X_train, X_test, y_train, y_test, batch_size, nb_epoch, Xs, Ys\n",
    "    \n",
    "# テキストを分かち書きして返す\n",
    "def tokenize(text):\n",
    "    wakati = MeCab.Tagger('-O wakati')\n",
    "    text = str(text).strip()\n",
    "    #print(text)\n",
    "    #encoded_text = text.encode('utf-8')\n",
    "    return wakati.parse(text)\n",
    " \n",
    "tokenized_text_list = [tokenize(texts) for texts in df[2]]\n",
    " \n",
    "max_features = 5000\n",
    "maxlen = 600\n",
    "batch_size = 32\n",
    "embedding_dim = 50\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 5\n",
    " \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_text_list)\n",
    "seq = tokenizer.texts_to_sequences(tokenized_text_list)\n",
    "X = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "Y = str(df[2].sample(n=10)).strip()\n",
    "\n",
    "#Y = str(df[2]).strip()\n",
    " \n",
    "# モデルを返すメソッド\n",
    "def build_model():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    " \n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen,\n",
    "                        dropout=0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn nb_filter\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "    # we use max pooling:\n",
    "    model.add(MaxPooling1D(pool_length=model.output_shape[1]))\n",
    " \n",
    "    # We flatten the output of the conv layer,\n",
    "    # so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    " \n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    " \n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    adam = Adam()\n",
    "    model.compile(loss='poisson',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train():\n",
    "    sequence_length, embedding_dim, filter_sizes, vocabulary_size, num_filters, drop, idx_name, \\\n",
    "    X_train, X_test, y_train, y_test, batch_size, nb_epoch, Xs, Ys = init_train()\n",
    "    model = build_model(sequence_length=sequence_length, \\\n",
    "    filter_sizes=filter_sizes, \\\n",
    "    embedding_dim=embedding_dim, \\\n",
    "    vocabulary_size=vocabulary_size, \\\n",
    "    num_filters=num_filters, \\\n",
    "    drop=drop, \\\n",
    "    idx_name=idx_name)\n",
    "    model.fit(Xs, Ys, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "    model.save('cnn_text_clsfic_all.model')\n",
    "    \n",
    "\n",
    "train()\n",
    "voc = pickle.loads(open('vod.pkl', 'rb').read())\n",
    "maxlen = voc['___META_MAXLEN___']\n",
    "maxwords = voc['___META_MAXWORD___']\n",
    "idx_name = pickle.loads(open('idx_name.pkl', 'rb').read())\n",
    "model = load_model('cnn_text_clsfic.model')\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    buff = [maxwords]*maxlen\n",
    "for i, ch in enumerate(line):\n",
    "    if voc.get(ch) is None:\n",
    "        buff[i] = maxwords\n",
    "    else:\n",
    "        buff[i] = voc.get(ch)\n",
    "results = model.predict(np.array([buff]), verbose=0)\n",
    "preds = results#np.log(results) / 1.0\n",
    "exp_preds = np.exp(preds)\n",
    "preds = exp_preds / np.sum(exp_preds)\n",
    "for result in results:\n",
    "    logsoftmax = np.log(result)\n",
    "    max_ent = list(sorted([(i,e) for i,e in enumerate(list(logsoftmax))], key=lambda x:x[1]*-1))[:10]\n",
    "    _, mini = min(max_ent, key=lambda x:x[1])\n",
    "    _, maxi = max(max_ent, key=lambda x:x[1])\n",
    "    base = maxi - mini\n",
    "    for ent in max_ent:\n",
    "        id, prob = ent\n",
    "        prob = (prob - mini)/base\n",
    "    if int(float(prob)*100) == 0: \n",
    "        prob += .01\n",
    "        #print(mini)\n",
    "        #print(prob)\n",
    "    print(idx_name.get(id).split('/').pop().split('.').pop(0), \"%d\"%(int(float(prob)*100)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
