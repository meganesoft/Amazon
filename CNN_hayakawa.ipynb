{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2243    明るさは滅びの姿であろうか。人も星も、暗いうちはまだ滅亡せぬ。\\r\\n――円城塔氏 推薦！\\...\n",
      "3004    「創造的破壊のドキュメンタリー。\\r\\n経済学に関心がない人にこそ読んでほしい、\\r\\n最上...\n",
      "1853    雪に覆われ下界と遮断されたシタフォード村の山荘。そこに集まった隣人たちが退屈しのぎに降霊会を...\n",
      "1428    〈ツイスト博士シリーズ〉内側から錠がかかった書斎で、ミステリ作家が煮えたぎる鍋に顔と両手を突...\n",
      "741     進化論から量子論、宇宙論まで20世紀科学の成果は広大だ。ガリレオの打ち立てた基幹から芽生え、...\n",
      "77      古き良きエドワード朝時代の面影を今なお残すバートラム・ホテル。ミス・マープルも淡い過去の思い...\n",
      "561     ミステリ史を語るうえで不可欠の名探偵たちが登場する作品を結集した名探偵アンソロジーの決定版第...\n",
      "1035    日本ＳＦ作家クラブ創立50周年記念特集\\r\\n1963年3月に創立され、2013年に50周年...\n",
      "1549    \\n大変ご好評によりハヤカワ・オンラインでの特典ポスターが先着購入枚数に達しましたため、10...\n",
      "1975    凄腕の殺し屋マイケルは、ガールフレンドのエレナの妊娠を機に、組織を抜けようと誓った。育ての親...\n",
      "Name: 2, dtype: object\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-79b7be125361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# evaluate using 10-fold cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, y, n_folds, shuffle, random_state)\u001b[0m\n\u001b[1;32m    538\u001b[0m             len(y), n_folds, shuffle, random_state)\n\u001b[1;32m    539\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0munique_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_inversed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mlabel_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_inversed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    " \n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    " \n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import numpy as np\n",
    " \n",
    "# 適当なデータを用意してください\n",
    "df = pd.read_csv('data/book.csv', header=None)\n",
    " \n",
    "#読み込んだcsvから長さ読み込んでもろもろの処理をするメソッドを書く\n",
    "def init_train():\n",
    "    print('Loading data')\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    voc = {}\n",
    "    maxlen = 0\n",
    "    maxwords = 0\n",
    "    buff = set()\n",
    "    TARGET_DIR = 'data/txt/*'\n",
    "    idx_name = {}\n",
    "    for i,filename in enumerate(df[3]):\n",
    "        idx_name[i] = filename\n",
    "    for line in df[2]:\n",
    "        a - list(line)\n",
    "    #行の最大文字数\n",
    "        maxlen = max(maxlen, len(a))\n",
    "        [buff.add(w) for w in a]\n",
    "    #行数\n",
    "    maxwords = len(buff)\n",
    "    #限界にMAX,それぞれの項目に数値を入れている\n",
    "    voc[maxwords] = '___MAX___'\n",
    "    voc['___META_MAXWORD___'] = maxwords\n",
    "    voc['___META_MAXLEN___'] = maxlen\n",
    "    print(\"maxwords %d\"%maxwords)\n",
    "    print(\"maxlen %d\"%maxlen)\n",
    "    print(\"idx name len %d\"%len(idx_name))\n",
    "    for i,filename in enumerate(df[3]):\n",
    "        for line in set(filter(lambda x:x!='',df[2])): \n",
    "            X = [maxwords]*maxlen\n",
    "            #余分な空白を消す\n",
    "            line = line.strip()\n",
    "            for idx, ch in enumerate(list(line)):\n",
    "            #辞書vocのch番目が殻だったらvocの長さを代入\n",
    "                if voc.get(ch) == None:\n",
    "                    voc[ch] = len(voc)\n",
    "                convert = voc[ch]\n",
    "                X[idx] = convert\n",
    "                #Xs,Ysそれぞれにvocの長さ,倍率を追加している\n",
    "        Xs.append(X)\n",
    "        y = [0.]*len(idx_name)\n",
    "        y[i] = 1.\n",
    "        Ys.append(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split( Xs, Ys, test_size=0.1, random_state=42)\n",
    "    open('vod.pkl', 'wb').write(pickle.dumps(voc))\n",
    "    open('idx_name.pkl', 'wb').write(pickle.dumps(idx_name))\n",
    "    sequence_length = maxlen\n",
    "    vocabulary_size = maxwords\n",
    "    embedding_dim   = 256*1\n",
    "    filter_sizes    = [3,4,5,1,2]\n",
    "    num_filters     = 512*1\n",
    "    drop            = 0.5\n",
    "    \n",
    "    nb_epoch   = 10\n",
    "    batch_size = 30\n",
    "    return sequence_length, embedding_dim, filter_sizes, vocabulary_size, num_filters, drop, idx_name, \\\n",
    "    X_train, X_test, y_train, y_test, batch_size, nb_epoch, Xs, Ys\n",
    "    \n",
    "# テキストを分かち書きして返す\n",
    "def tokenize(text):\n",
    "    wakati = MeCab.Tagger('-O wakati')\n",
    "    text = str(text).strip()\n",
    "    #print(text)\n",
    "    #encoded_text = text.encode('utf-8')\n",
    "    return wakati.parse(text)\n",
    " \n",
    "tokenized_text_list = [tokenize(texts) for texts in df[2]]\n",
    " \n",
    "max_features = 5000\n",
    "maxlen = 600\n",
    "batch_size = 32\n",
    "embedding_dim = 50\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 5\n",
    " \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_text_list)\n",
    "seq = tokenizer.texts_to_sequences(tokenized_text_list)\n",
    "X = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "Y = str(df[2].sample(n=10)).strip()\n",
    "\n",
    "#Y = str(df[2]).strip()\n",
    " \n",
    "# モデルを返すメソッド\n",
    "def build_model():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    " \n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen,\n",
    "                        dropout=0.2))\n",
    " \n",
    "    # we add a Convolution1D, which will learn nb_filter\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "    # we use max pooling:\n",
    "    model.add(MaxPooling1D(pool_length=model.output_shape[1]))\n",
    " \n",
    "    # We flatten the output of the conv layer,\n",
    "    # so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    " \n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    " \n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    " \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "model = KerasClassifier(build_fn=build_model, nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "# evaluate using 10-fold cross validation\n",
    "print(Y)\n",
    "kfold = StratifiedKFold(y=Y, n_folds=10, shuffle=True)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
